# Monitoring and Alerts

The bot exposes Prometheus metrics on `/metrics`. Key metrics include:

* `cpu_usage_pct` – process CPU utilisation
* `rss_usage_mb` – resident memory usage
* `queue_depth` – pending messages in the signal queue
* `trade_count` – total trades executed
* `drift_events` – detected model drift events

## InfluxDB bootstrap

Use the deployment helper to provision the organisation, bucket and scoped
token that the bot uses for metrics ingestion:

```bash
python -m deployment.influx_bootstrap \
  --url http://influxdb:8086 \
  --org trading \
  --bucket mt5_metrics \
  --env-file deploy/secrets/influx.env \
  --print-exports
```

The script inspects the InfluxDB API to determine whether it needs to run the
initial `/api/v2/setup` flow or simply ensure the bucket exists.  A bucket
scoped token is created (or reused) and stored in `deploy/secrets/influx.env`,
which systemd loads automatically via `EnvironmentFile`.  If you need to run
the helper as part of provisioning, expose the following environment variables
before executing `scripts/install_service.sh`:

* `INFLUXDB_BOOTSTRAP_URL`
* `INFLUXDB_BOOTSTRAP_ORG`
* `INFLUXDB_BOOTSTRAP_BUCKET`

Optional flags such as `INFLUXDB_BOOTSTRAP_USERNAME`,
`INFLUXDB_BOOTSTRAP_RETENTION`, and `INFLUXDB_BOOTSTRAP_ADMIN_TOKEN` allow the
automation to customise credentials and retention policies.  By default the
script refuses to overwrite existing secrets, so rerunning the installer is
safe.

## Runtime secrets bootstrap

Configuration service API keys, local controller shared secrets and the AES
material used to encrypt checkpoints are generated by
`deployment/runtime_secrets.py`.  The helper mirrors the Influx automation and
persists secrets to `deploy/secrets/runtime.env`, which the systemd unit loads
via `EnvironmentFile`:

```bash
python -m deployment.runtime_secrets \
  --env-file deploy/secrets/runtime.env \
  --print-exports
```

The script is idempotent: rerunning it reuses the values already present in the
target file unless `--rotate NAME` or `--force` is provided.  During unattended
installs `scripts/install_service.sh` executes the helper automatically; set
`RUNTIME_SECRETS_SKIP=1` to disable the step or pass
`RUNTIME_SECRETS_ROTATE="API_KEY"` to rotate specific entries while keeping the
rest untouched.

## Prometheus

A sample configuration is provided in `deploy/prometheus.yml`:

```bash
prometheus --config.file=deploy/prometheus.yml
```

### Endpoint provisioning

`scripts/install_service.sh` now invokes `deployment/prometheus_endpoints.py`
automatically so you do not need to manage Prometheus URLs manually. The helper
derives `PROM_PUSH_URL` and `PROM_QUERY_URL` using the provided host/port flags
and writes them into `deploy/secrets/runtime.env` alongside the other generated
secrets. Set environment overrides such as `PROM_ENDPOINTS_PUSH_HOST` before
running the installer to customise the generated URLs, or rerun the helper with
`--force` when you need to change them later.

## Alerting

Optional alerting rules are defined in `deploy/alert.rules.yml`. Enable them by
referencing the file from your Prometheus configuration and configuring
Alertmanager receivers:

```yaml
rule_files:
  - alert.rules.yml
```

Example alerts cover sustained CPU usage over 90% and broker reconnection
attempts. Configure Alertmanager as usual to deliver notifications.

## Log Forwarding

Set `log_forward.url` in `config.yaml` to stream JSON logs to an external
aggregator such as ELK or Datadog. HTTP endpoints and `syslog://host:port`
targets are supported. The bot verifies connectivity on startup and falls
back to local file logging if unreachable. Handlers buffer messages and retry
so temporary network outages do not drop logs.

## Diagnostics

Nightly diagnostics record the Python environment and dataset hashes. The
scheduler invokes `scripts/diagnostics.sh`, which writes reports to
`logs/diagnostics/`. Any dependency or data mismatches trigger alerts with
instructions for syncing via `pip install -r requirements.txt` or `python
scripts/check_data_versions.py`.
