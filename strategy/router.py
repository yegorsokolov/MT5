from __future__ import annotations

"""Contextual bandit based strategy router.

This module implements a lightweight LinUCB contextual bandit that chooses
between different trading algorithms based on regime features such as market
volatility, trend strength and a numeric market regime indicator.  After each
trade the realised profit and loss is fed back to the bandit so allocation
gradually shifts toward the best-performing algorithm in the current regime.
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Tuple, Optional

from datetime import datetime

import logging
import numpy as np
import pandas as pd

from analysis.algorithm_rating import load_ratings
from analysis.rationale_scorer import load_algorithm_win_rates
from analytics.regime_performance_store import RegimePerformanceStore
from analytics.metrics_aggregator import record_metric
from state_manager import load_router_state, save_router_state
from models.ftrl import FTRLModel
from utils.resource_monitor import monitor
try:  # optional dependency - statsmodels may be missing
    from .pair_trading import signal_from_features as pair_signal
except Exception:  # pragma: no cover
    def pair_signal(*_, **__):
        return 0.0
from .fuzzy_consensus import FuzzyConsensus

try:  # optional dependency - meta controller may not be available
    from rl.meta_controller import MetaController
except Exception:  # pragma: no cover - best effort
    MetaController = None  # type: ignore

try:  # pragma: no cover - knowledge graph is optional
    from analysis.knowledge_graph import (
        load_knowledge_graph,
        opportunity_score,
        risk_score,
    )
except Exception:  # pragma: no cover - fall back to no-op implementations
    def load_knowledge_graph(*_, **__):  # type: ignore
        return None

    def risk_score(*_, **__):  # type: ignore
        return 0.0

    def opportunity_score(*_, **__):  # type: ignore
        return 0.0

FeatureDict = Dict[str, float]
Algorithm = Callable[[FeatureDict], float]


def _feature_vector(features: FeatureDict, factor_names: Iterable[str]) -> np.ndarray:
    """Return feature vector including factor exposures.

    The base features are volatility, trend strength, regime and optional
    graph-derived risk/opportunity signals.  Any factor exposures provided
    under the ``factor_exposures`` key are appended in the order of
    ``factor_names``.  Missing factors default to zero exposure.
    """

    exposures = features.get("factor_exposures", {}) or {}
    vec: List[float] = [
        features.get("volatility", 0.0),
        features.get("trend_strength", 0.0),
        features.get("regime", 0.0),
        features.get("graph_risk", 0.0),
        features.get("graph_opportunity", 0.0),
    ]
    for name in factor_names:
        vec.append(float(exposures.get(name, 0.0)))
    return np.array(vec).reshape(-1, 1)


@dataclass
class StrategyRouter:
    """Route orders between algorithms using a LinUCB contextual bandit."""

    algorithms: Dict[str, Algorithm] = field(default_factory=dict)
    alpha: float = 0.1
    consensus_threshold: float = 0.6
    consensus: FuzzyConsensus = field(default_factory=FuzzyConsensus)
    scoreboard_path: Path | str = Path("reports/strategy_scores.parquet")
    elo_path: Path | str = Path("reports/elo_ratings.parquet")
    rationale_path: Path | str = Path(
        "reports/rationale_scores/algorithm_win_rates.parquet"
    )
    regime_perf_path: Path | str = Path("analytics/regime_performance.parquet")
    factor_names: List[str] = field(default_factory=list)
    # Optional meta-controller and underlying RL agents
    meta_controller: Optional[MetaController] = None
    rl_agents: Dict[str, Algorithm] = field(default_factory=dict)
    use_ftrl: bool = field(default=False, init=False)
    ftrl_models: Dict[str, FTRLModel] = field(default_factory=dict, init=False)
    promotion_thresholds: Dict[str, float] = field(default_factory=dict)
    demotion_thresholds: Dict[str, float] = field(default_factory=dict)
    evaluation_path: Path | str = Path("reports/router_evaluations.csv")

    def __post_init__(self) -> None:
        self.logger = logging.getLogger(__name__)
        if not self.algorithms:
            # Default placeholder algorithms.  They simply return a constant
            # action; real applications should supply concrete implementations.
            self.algorithms = {
                "mean_reversion": lambda _: -1.0,
                "trend_following": lambda _: 1.0,
                # rl_policy will be replaced by meta-controller if provided
                "rl_policy": lambda _: 0.0,
                # pair trading acts on features generated by strategy.pair_trading
                "pair_trading": pair_signal,
            }
        # If a meta-controller and RL agents are supplied, expose a single
        # aggregated RL policy using the controller's allocation weights.
        if self.meta_controller and self.rl_agents:
            self.algorithms["rl_policy"] = self._meta_rl_signal
        for name in self.algorithms:
            self.consensus.weights.setdefault(name, 1.0)
        # include regime dimension alongside volatility, trend strength,
        # graph signals and optional factor exposures
        self.dim = 5 + len(self.factor_names)
        self.A: Dict[str, np.ndarray] = {
            name: np.identity(self.dim) for name in self.algorithms
        }
        self.b: Dict[str, np.ndarray] = {
            name: np.zeros((self.dim, 1)) for name in self.algorithms
        }
        self.history: List[Tuple[FeatureDict, float, str]] = []
        self.scoreboard_path = Path(self.scoreboard_path)
        self.scoreboard = self._load_scoreboard()
        self.elo_path = Path(self.elo_path)
        self.elo_ratings = self._load_elo_ratings()
        self.rationale_path = Path(self.rationale_path)
        self.rationale_scores = self._load_rationale_scores()
        self.regime_perf_path = Path(self.regime_perf_path)
        self.regime_performance = self._load_regime_performance()
        self.reward_sums: Dict[str, float] = {name: 0.0 for name in self.algorithms}
        self.plays: Dict[str, int] = {name: 0 for name in self.algorithms}
        self.total_plays: int = 0
        self.champion: str | None = None
        self._load_state()
        if self.champion is None and self.algorithms:
            self.champion = next(iter(self.algorithms))

        # Load knowledge graph if available
        try:  # pragma: no cover - best effort
            self.knowledge_graph = load_knowledge_graph()
        except Exception:
            self.knowledge_graph = None

        # Determine whether to use lightweight FTRL models based on capability tier
        if monitor and getattr(monitor, "capability_tier", "") == "lite":
            self.use_ftrl = True
            self.logger.info("Initialising FTRL models for lite capability tier")
            self.ftrl_models = {name: FTRLModel(dim=self.dim) for name in self.algorithms}
        else:
            self.use_ftrl = False

    # ------------------------------------------------------------------
    def _augment_with_graph(self, features: FeatureDict) -> None:
        """Augment ``features`` with graph-derived signals if possible."""
        company = features.get("company") or features.get("asset")
        if not company or self.knowledge_graph is None:
            return
        try:  # pragma: no cover - best effort
            features["graph_risk"] = risk_score(self.knowledge_graph, str(company))
            features["graph_opportunity"] = opportunity_score(
                self.knowledge_graph, str(company)
            )
        except Exception:
            features.setdefault("graph_risk", 0.0)
            features.setdefault("graph_opportunity", 0.0)

    # ------------------------------------------------------------------
    def _meta_rl_signal(self, features: FeatureDict) -> float:
        """Blend underlying RL agent signals using the meta-controller."""
        if not self.meta_controller:
            # Should not happen but return neutral action as fallback
            return 0.0
        returns = np.array([alg(features) for alg in self.rl_agents.values()], dtype=float)
        state_vec = _feature_vector(features, self.factor_names).ravel()
        try:
            weights = self.meta_controller.predict(returns, state_vec)
        except Exception:  # pragma: no cover - best effort
            weights = np.ones_like(returns) / max(len(returns), 1)
        return float(np.dot(weights, returns))

    def _maybe_upgrade_models(self) -> None:
        """Switch back to full models if resource tier improves."""
        if self.use_ftrl and monitor and getattr(monitor, "capability_tier", "lite") != "lite":
            self.logger.info(
                "Capability tier upgraded to %s; switching to full models", monitor.capability_tier
            )
            self.use_ftrl = False
            self.ftrl_models.clear()

    def set_factor_names(self, names: Iterable[str]) -> None:
        """Configure factor names and expand internal matrices if required."""
        new_names = list(names)
        if new_names == self.factor_names:
            return
        old_dim = self.dim
        self.factor_names = new_names
        self.dim = 5 + len(self.factor_names)
        if self.dim == old_dim:
            return
        for key in list(self.A.keys()):
            A_old = self.A[key]
            b_old = self.b[key]
            A_new = np.identity(self.dim)
            A_new[:old_dim, :old_dim] = A_old
            b_new = np.zeros((self.dim, 1))
            b_new[:old_dim] = b_old
            self.A[key] = A_new
            self.b[key] = b_new

    # Registration -----------------------------------------------------
    def register(self, name: str, algorithm: Algorithm) -> None:
        """Register a new algorithm with fresh bandit parameters."""
        self.algorithms[name] = algorithm
        self.A[name] = np.identity(self.dim)
        self.b[name] = np.zeros((self.dim, 1))
        self.reward_sums[name] = 0.0
        self.plays[name] = 0
        self.consensus.weights.setdefault(name, 1.0)
        self._save_state()
        try:
            from core.orchestrator import GLOBAL_ORCHESTRATOR

            if GLOBAL_ORCHESTRATOR is not None:
                GLOBAL_ORCHESTRATOR.register_strategy(name, algorithm)
        except Exception:  # pragma: no cover - best effort
            pass

    def promote(self, name: str, algorithm: Algorithm) -> None:
        """Expose ``algorithm`` to live trading if not already present."""
        if name in self.algorithms:
            return
        self.register(name, algorithm)
        try:  # pragma: no cover - metrics aggregation is optional
            record_metric("strategy_promoted", 1.0, tags={"name": name})
        except Exception:
            pass

    def demote(self, name: str) -> None:
        """Remove ``name`` from live trading and bandit rotation."""
        if name not in self.algorithms:
            return
        self.algorithms.pop(name, None)
        self.A.pop(name, None)
        self.b.pop(name, None)
        self.reward_sums.pop(name, None)
        self.plays.pop(name, None)
        self.consensus.weights.pop(name, None)
        try:  # pragma: no cover - metrics aggregation is optional
            record_metric("strategy_demoted", 1.0, tags={"name": name})
        except Exception:
            pass
        self._save_state()

    def _save_state(self) -> None:
        try:
            save_router_state(
                self.champion,
                self.A,
                self.b,
                self.reward_sums,
                self.plays,
                self.history,
                self.total_plays,
            )
        except Exception:  # pragma: no cover - best effort
            pass

    # Evaluation -------------------------------------------------------
    def _persist_evaluation(self, rec: Dict[str, Any]) -> None:
        path = Path(self.evaluation_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        header = not path.exists()
        with path.open("a") as f:
            if header:
                f.write(
                    "timestamp,name,pnl,drawdown,slippage,fill_ratio,violations\n"
                )
            f.write(
                f"{rec.get('timestamp')},{rec.get('name')},{rec.get('pnl',0.0):.6f},{rec.get('drawdown',0.0):.6f},{rec.get('slippage',0.0):.6f},{rec.get('fill_ratio',0.0):.6f},{rec.get('violations','')}\n"
            )

    def evaluate(
        self,
        name: str,
        metrics: Dict[str, float],
        algorithm: Algorithm | None = None,
    ) -> None:
        """Record live metrics and promote/demote strategies if thresholds hit."""
        rec: Dict[str, Any] = {
            "timestamp": datetime.utcnow().isoformat(),
            "name": name,
            "pnl": float(metrics.get("pnl", 0.0)),
            "drawdown": float(metrics.get("drawdown", 0.0)),
            "slippage": float(metrics.get("slippage", 0.0)),
            "fill_ratio": float(metrics.get("fill_ratio", 0.0)),
        }
        violations: List[str] = []
        if name not in self.algorithms:
            if algorithm is not None and all(
                metrics.get(k, float("-inf")) >= v
                for k, v in self.promotion_thresholds.items()
            ):
                self.promote(name, algorithm)
                try:
                    record_metric("router_promotion", 1.0, {"name": name})
                except Exception:
                    pass
        else:
            for k, v in self.demotion_thresholds.items():
                val = metrics.get(k)
                if val is None:
                    continue
                if k in {"drawdown", "slippage"}:
                    if val > v:
                        violations.append(k)
                else:
                    if val < v:
                        violations.append(k)
            if violations:
                rec["violations"] = ";".join(violations)
                self.demote(name)
                try:
                    record_metric(
                        "router_violation", 1.0, {"name": name, "type": ",".join(violations)}
                    )
                except Exception:
                    pass
        self._persist_evaluation(rec)
        try:
            tags = {"name": name}
            record_metric("router_pnl", rec["pnl"], tags)
            record_metric("router_drawdown", rec["drawdown"], tags)
            record_metric("router_slippage", rec["slippage"], tags)
            record_metric("router_fill_ratio", rec["fill_ratio"], tags)
        except Exception:
            pass

    def _load_state(self) -> None:
        state = load_router_state()
        if not state:
            return
        self.champion = state.get("champion", self.champion)
        self.A.update(state.get("A", {}))
        self.b.update(state.get("b", {}))
        self.reward_sums.update(state.get("rewards", {}))
        self.plays.update(state.get("counts", {}))
        self.history = state.get("history", self.history)
        self.total_plays = state.get("total_plays", self.total_plays)

    def _reward_ucb(self, name: str) -> float:
        n = self.plays.get(name, 0)
        if n == 0:
            return 0.0
        mean = self.reward_sums.get(name, 0.0) / n
        bonus = self.alpha * np.sqrt(2.0 * np.log(self.total_plays + 1) / n)
        return mean + bonus

    # Selection --------------------------------------------------------
    def select(self, features: FeatureDict) -> str:
        """Return the algorithm name with the highest score."""
        self._maybe_upgrade_models()
        self._augment_with_graph(features)
        x = _feature_vector(features, self.factor_names)
        if self.use_ftrl:
            x_vec = x.ravel()
            best_name = max(
                self.algorithms.keys(), key=lambda n: self.ftrl_models[n].predict(x_vec)
            )
            if best_name != self.champion:
                self.champion = best_name
                self._save_state()
            return best_name

        regime = features.get("regime")
        basket = features.get("market_basket")
        instrument = features.get("instrument")
        best_name = self.champion
        best_score = -np.inf
        for name in self.algorithms:
            A_inv = np.linalg.inv(self.A[name])
            theta = A_inv @ self.b[name]
            mean = float((theta.T @ x).item())
            bonus = float(self.alpha * np.sqrt((x.T @ A_inv @ x).item()))
            score = (
                mean
                + bonus
                + self._reward_ucb(name)
                + self._scoreboard_weight(instrument, basket, name)
                + self._elo_weight(name)
                + self._rationale_weight(name)
                + self._regime_performance_weight(regime, name)
            )
            if score > best_score:
                best_score = score
                best_name = name
        assert best_name is not None  # for type checkers
        if best_name != self.champion:
            self.champion = best_name
            self._save_state()
        return best_name

    def act(self, features: FeatureDict) -> Tuple[str, float]:
        """Select an algorithm and return its action gated by consensus."""
        signals = {n: alg(features) for n, alg in self.algorithms.items()}
        consensus_score, _ = self.consensus.score(signals)
        name = self.select(features)
        action = signals[name] if consensus_score >= self.consensus_threshold else 0.0
        self.logger.info("Consensus score %.3f for %s", consensus_score, name)
        return name, action

    # Learning ---------------------------------------------------------
    def update(
        self,
        features: FeatureDict,
        reward: float,
        algorithm: str,
        *,
        smooth: float = 0.1,
    ) -> None:
        """Update model parameters with observed ``reward``.

        ``smooth`` controls the exponential smoothing factor applied when
        updating risk-adjusted scoreboard metrics.  A value of ``0`` keeps the
        previous offline metric, while ``1`` replaces it entirely with the
        latest observation.
        """

        self._maybe_upgrade_models()
        x = _feature_vector(features, self.factor_names)
        if self.use_ftrl:
            y = 1.0 if reward > 0 else 0.0
            self.ftrl_models[algorithm].update(x.ravel(), y)
        else:
            self.A[algorithm] += x @ x.T
            self.b[algorithm] += reward * x
        self.history.append((features, reward, algorithm))
        self.reward_sums[algorithm] += reward
        self.plays[algorithm] += 1
        self.total_plays += 1
        self._update_scoreboard(
            features.get("instrument"), features.get("market_basket"), algorithm, smooth
        )
        self._save_state()

    # Convenience ------------------------------------------------------
    def log_reward(self, features: FeatureDict, reward: float, algorithm: str) -> None:
        """Alias for :meth:`update` for semantic clarity."""
        self.update(features, reward, algorithm)

    # Scoreboard -------------------------------------------------------
    def _load_scoreboard(self) -> pd.DataFrame:
        if self.scoreboard_path.exists():
            return pd.read_parquet(self.scoreboard_path)
        return pd.DataFrame(
            columns=["pnl", "sharpe", "drawdown"],
            index=pd.MultiIndex.from_tuples(
                [], names=["instrument", "market_basket", "algorithm"]
            ),
        )

    def _scoreboard_weight(
        self, instrument: float | int | str | None, basket: float | int | None, algorithm: str
    ) -> float:
        if instrument is None or basket is None or self.scoreboard.empty:
            return 0.0
        try:
            val = float(self.scoreboard.loc[(instrument, basket, algorithm), "sharpe"])
            if abs(val) > 1e3:
                return 0.0
            return val
        except KeyError:
            return 0.0

    def _load_elo_ratings(self) -> Dict[str, float]:
        """Load persisted ELO ratings."""
        try:
            return load_ratings(self.elo_path).to_dict()
        except Exception:
            return {}

    def _elo_weight(self, algorithm: str) -> float:
        """Return a small weight based on the algorithm's ELO rating."""
        rating = self.elo_ratings.get(algorithm)
        if rating is None:
            return 0.0
        return (float(rating) - 1500.0) / 400.0

    def _load_rationale_scores(self) -> Dict[str, float]:
        """Load win rates derived from rationale scoring."""
        try:
            return load_algorithm_win_rates(self.rationale_path)
        except Exception:
            return {}

    def _rationale_weight(self, algorithm: str) -> float:
        """Return a small bonus for algorithms with strong rationales."""
        rate = self.rationale_scores.get(algorithm)
        if rate is None:
            return 0.0
        return float(rate) - 0.5

    def _load_regime_performance(self) -> pd.DataFrame:
        """Load historical PnL by regime and model."""
        try:
            store = RegimePerformanceStore(self.regime_perf_path)
            return store.load()
        except Exception:
            return pd.DataFrame(
                columns=["date", "model", "regime", "pnl_daily", "pnl_weekly"]
            )

    def refresh_regime_performance(self) -> None:
        """Reload regime performance statistics from disk."""
        self.regime_performance = self._load_regime_performance()

    def _regime_performance_weight(self, regime: float | None, algorithm: str) -> float:
        """Return a normalised weight based on historical PnL."""
        if regime is None or self.regime_performance.empty:
            return 0.0
        df = self.regime_performance
        sub = df[(df["regime"] == regime) & (df["model"] == algorithm)]
        if sub.empty:
            return 0.0
        pnl = float(sub.iloc[-1]["pnl_weekly"])
        scale = float(df["pnl_weekly"].abs().max() or 1.0)
        return pnl / scale

    def _update_scoreboard(
        self,
        instrument: float | int | str | None,
        basket: float | int | None,
        algorithm: str,
        smooth: float,
    ) -> None:
        """Blend offline metrics with live PnL using exponential smoothing."""
        if instrument is None or basket is None:
            return
        rewards = [
            r
            for f, r, a in self.history
            if a == algorithm
            and f.get("instrument") == instrument
            and f.get("market_basket") == basket
        ]
        if not rewards:
            return
        arr = np.asarray(rewards)
        live_sharpe = float(arr.mean() / (arr.std() + 1e-9))
        prev = 0.0
        try:
            prev = float(self.scoreboard.loc[(instrument, basket, algorithm), "sharpe"])
        except KeyError:
            pass
        sharpe = smooth * live_sharpe + (1.0 - smooth) * prev
        cumulative = (1 + arr).cumprod()
        drawdown = float((np.maximum.accumulate(cumulative) - cumulative).max())
        self.scoreboard.loc[(instrument, basket, algorithm), ["pnl", "sharpe", "drawdown"]] = [
            float(arr.sum()),
            sharpe,
            drawdown,
        ]
        tags = {"instrument": instrument, "basket": basket, "algorithm": algorithm}
        try:
            record_metric("strategy_pnl", float(arr.sum()), tags)
            record_metric("strategy_sharpe", sharpe, tags)
            record_metric("strategy_drawdown", drawdown, tags)
        except Exception:
            pass
        try:
            self.scoreboard_path.parent.mkdir(parents=True, exist_ok=True)
            self.scoreboard.to_parquet(self.scoreboard_path)
        except Exception:
            pass


__all__ = ["StrategyRouter"]
